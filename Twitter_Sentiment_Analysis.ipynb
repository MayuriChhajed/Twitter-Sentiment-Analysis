{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Jo5QEZPFqY",
        "colab_type": "text"
      },
      "source": [
        "**Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGFgqVwTXSk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d39440d1-d978-4a89-dc0c-1239b0ca3a36"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0fdwXjrhYlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "adb29e2a-dab7-423b-f64b-e871ac7dfbb3"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/twitter sentiment analysis/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/twitter sentiment analysis/test.csv')\n",
        "print(train.head())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  label                                              tweet\n",
            "0   1      0   @user when a father is dysfunctional and is s...\n",
            "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
            "2   3      0                                bihday your majesty\n",
            "3   4      0  #model   i love u take with u all the time in ...\n",
            "4   5      0             factsguide: society now    #motivation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Fut8SeOZGL",
        "colab_type": "text"
      },
      "source": [
        "**Tweets Pre-processing and cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6o_sUxdHDoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine train and test for data cleaning\n",
        "combi = train\n",
        "combi = combi.append(test, ignore_index=True)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me_10YVtPT8v",
        "colab_type": "text"
      },
      "source": [
        "Removing twitter handles (@user)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raOzx3WKh3aT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt\n",
        "combi['clean'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkvSbxuaPkH7",
        "colab_type": "text"
      },
      "source": [
        "Removing punctuations, numbers and special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YNJB1YLmQLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['clean'] = combi['clean'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_KnnJ66PzU2",
        "colab_type": "text"
      },
      "source": [
        "Removing short words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vd8wJ1zm5gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combi['clean'] = combi['clean'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpq8TuReQOHh",
        "colab_type": "text"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJlZ-16kuUnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_tweet = combi['clean'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head()\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "tokenized_tweet.head()\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "\n",
        "combi['new'] = tokenized_tweet"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIbucmkfQ6lp",
        "colab_type": "text"
      },
      "source": [
        "**Extracting features from cleaned tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMWgDy6Iu6ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words = ' '.join([text for text in combi['new']])\n",
        "words = all_words.split()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnjqvEcs8Kal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split train and test after data cleaning\n",
        "train['new'] = combi['new'][0:31962]\n",
        "test = combi[:][31962:]\n",
        "\n",
        "#print(train.shape,test.shape)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0agqbvZXAwZ",
        "colab_type": "text"
      },
      "source": [
        "Encoding Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL1P7decpCND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "reviews_int = []\n",
        "\n",
        "for review in train['new']:\n",
        "  reviews_int.append([vocab_to_int[word] for word in review.split()])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS5eYpjrxy-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_labels = np.array([1 if i == 1 else 0 for i in train['label']])"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8H1cIm9XS-H",
        "colab_type": "text"
      },
      "source": [
        "Removing Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2vIDsLIypDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ca55a919-9214-4b08-d82e-2d1577f2fedb"
      },
      "source": [
        "review_lens = Counter([len(x) for x in reviews_int])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 51\n",
            "Maximum review length: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxF4LNBXy1rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3399130d-b011-4f42-f713-c8b6e3a18f48"
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_int))\n",
        "\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_int) if len(review) != 0]\n",
        "\n",
        "reviews_int = [reviews_int[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_int))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  31962\n",
            "Number of reviews after removing outliers:  31911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT1ohUVzXxka",
        "colab_type": "text"
      },
      "source": [
        "Padding sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDt70mN60Qg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(reviews_ints):\n",
        "      if len(row)>0:\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXZealWi0vrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 25\n",
        "\n",
        "features = pad_features(reviews_int, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_int), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\""
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvzmG9wLYRnI",
        "colab_type": "text"
      },
      "source": [
        "Training, Validation, Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgIsZRUs1PuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "088d3d04-d1be-45db-dc62-cc8f0a17235a"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(25528, 25) \n",
            "Validation set: \t(3191, 25) \n",
            "Test set: \t\t(3192, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B36PQkbaYfzE",
        "colab_type": "text"
      },
      "source": [
        "Dataloaders and Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wo2jfti1xhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 128\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26f--RXD2BVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "50599b25-ceaf-4e53-dee3-88256b19f002"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([128, 25])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,    33,  1636,  3635],\n",
            "        [    0,     0,     0,  ...,   238,     2, 10655],\n",
            "        [    0,     0,     0,  ...,   731, 17917,  1553],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,     4,  1727,    28],\n",
            "        [    0,     0,     0,  ...,    61,  3370,  2756],\n",
            "        [    0,     0,     0,  ...,   159,    24,    11]])\n",
            "\n",
            "Sample label size:  torch.Size([128])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxUH5_rGYvuW",
        "colab_type": "text"
      },
      "source": [
        "**Sentiment Network with Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5rHmaIv2IRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cca046ce-9b57-4994-e7af-2eb2bba4752d"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PJDqJ72TqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC-VE1R320j7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c72f31ca-8b4b-4c68-e32d-31c2ff6a932e"
      },
      "source": [
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 3\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(46476, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVpGFjNZOhP",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H35ULjI127lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBrV1MHo29Z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "1ee723ca-ff24-44eb-f26b-b651ee16683b"
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 3 # 2-3 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/3... Step: 100... Loss: 0.257439... Val Loss: 0.183304\n",
            "Epoch: 2/3... Step: 200... Loss: 0.105234... Val Loss: 0.150100\n",
            "Epoch: 2/3... Step: 300... Loss: 0.053264... Val Loss: 0.129974\n",
            "Epoch: 3/3... Step: 400... Loss: 0.074059... Val Loss: 0.132694\n",
            "Epoch: 3/3... Step: 500... Loss: 0.033075... Val Loss: 0.150727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APVDzqpVc4WJ",
        "colab_type": "text"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rttY82Kk55fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "04044191-fb79-4c3a-c9dc-93a19d78406d"
      },
      "source": [
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.162\n",
            "Test accuracy: 0.918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzHyEJNVc7Wf",
        "colab_type": "text"
      },
      "source": [
        "Inference on original test review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1E4Hv3uiaGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, sequence_length=25):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze()) \n",
        "    if pred == 0:\n",
        "      return 0\n",
        "    elif pred==1:\n",
        "      return 1\n",
        "    return pred"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvt_9pHEind6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = []\n",
        "\n",
        "seq_length=25 # good to use the length that was trained on\n",
        "\n",
        "#predict(net, test_review, seq_length)\n",
        "test.head()\n",
        "\n",
        "for i in test['new']:\n",
        "  output.append(predict(net,i,25))\n",
        "\n",
        "out = np.array(output)\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rlwQAqlzhfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = pd.read_csv('/content/drive/My Drive/twitter sentiment analysis/sample.csv')\n",
        "sample['label'] = out\n",
        "sample.to_csv('/content/drive/My Drive/twitter sentiment analysis/output4.csv', header=True, index=False)"
      ],
      "execution_count": 93,
      "outputs": []
    }
  ]
}